{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb0e9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set up plotting parameters\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0ad30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample movie reviews dataset\n",
    "sample_reviews = [\n",
    "    \"This movie is absolutely fantastic and amazing\",\n",
    "    \"I loved this film, it was great and wonderful\",\n",
    "    \"Terrible movie, very bad and disappointing\",\n",
    "    \"Amazing cinematography and excellent acting performance\",\n",
    "    \"Boring film with poor storyline and bad acting\",\n",
    "    \"Outstanding movie with brilliant direction and script\",\n",
    "    \"Worst movie ever, completely waste of time\",\n",
    "    \"Incredible story with fantastic visual effects\",\n",
    "    \"Poor quality film with terrible sound effects\",\n",
    "    \"Excellent movie with great character development\",\n",
    "    \"Awful acting and boring plot throughout\",\n",
    "    \"Masterpiece with outstanding performances by all actors\",\n",
    "    \"Disappointing film with weak storyline and direction\",\n",
    "    \"Brilliant movie with amazing special effects\",\n",
    "    \"Terrible script and poor character development\"\n",
    "]\n",
    "\n",
    "# Create corresponding labels (1 for positive, 0 for negative)\n",
    "labels = [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'review': sample_reviews,\n",
    "    'sentiment': labels\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7909972",
   "metadata": {},
   "source": [
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ecdff8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer with basic parameters\n",
    "count_vectorizer = CountVectorizer(\n",
    "    lowercase=True,           # Convert to lowercase\n",
    "    stop_words='english',     # Remove English stop words\n",
    "    max_features=100          # Limit to top 100 features\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "bow_matrix = count_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "print(\"Bag of Words Matrix Information:\")\n",
    "print(f\"Matrix shape: {bow_matrix.shape}\")\n",
    "print(f\"Matrix type: {type(bow_matrix)}\")\n",
    "print(f\"Matrix density: {bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1]):.4f}\")\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nVocabulary size: {len(feature_names)}\")\n",
    "print(f\"First 20 features: {feature_names[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb15f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix to dense for visualization\n",
    "bow_dense = bow_matrix.toarray()\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_dense, columns=feature_names)\n",
    "\n",
    "print(\"BoW Matrix (first 5 documents, first 10 features):\")\n",
    "print(bow_df.iloc[:5, :10])\n",
    "\n",
    "# Show word frequencies across all documents\n",
    "word_frequencies = np.sum(bow_dense, axis=0)\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_frequencies\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 most frequent words:\")\n",
    "print(word_freq_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a8c14c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot top 20 most frequent words\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_words = word_freq_df.head(20)\n",
    "plt.bar(range(len(top_words)), top_words['frequency'])\n",
    "plt.xticks(range(len(top_words)), top_words['word'], rotation=45, ha='right')\n",
    "plt.title('Top 20 Most Frequent Words in BoW')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create word cloud\n",
    "wordcloud_text = ' '.join(df['review'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(wordcloud_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Movie Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f12b5d",
   "metadata": {},
   "source": [
    "Implementing TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9303b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    max_features=100,\n",
    "    ngram_range=(1, 1)  # Only unigrams for now\n",
    ")\n",
    "\n",
    "# Fit and transform the text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['review'])\n",
    "\n",
    "print(\"TF-IDF Matrix Information:\")\n",
    "print(f\"Matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"Matrix type: {type(tfidf_matrix)}\")\n",
    "print(f\"Matrix density: {tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1]):.4f}\")\n",
    "\n",
    "# Get feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"\\nVocabulary size: {len(tfidf_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174cc090",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert TF-IDF matrix to dense\n",
    "tfidf_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'bow_frequency': word_frequencies,\n",
    "    'tfidf_mean': np.mean(tfidf_dense, axis=0)\n",
    "}).sort_values('bow_frequency', ascending=False)\n",
    "\n",
    "print(\"BoW vs TF-IDF Comparison (Top 15 words):\")\n",
    "print(comparison_df.head(15))\n",
    "\n",
    "# Visualize the comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# BoW frequencies\n",
    "top_15 = comparison_df.head(15)\n",
    "ax1.bar(range(len(top_15)), top_15['bow_frequency'])\n",
    "ax1.set_xticks(range(len(top_15)))\n",
    "ax1.set_xticklabels(top_15['word'], rotation=45, ha='right')\n",
    "ax1.set_title('BoW: Word Frequencies')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# TF-IDF scores\n",
    "ax2.bar(range(len(top_15)), top_15['tfidf_mean'])\n",
    "ax2.set_xticks(range(len(top_15)))\n",
    "ax2.set_xticklabels(top_15['word'], rotation=45, ha='right')\n",
    "ax2.set_title('TF-IDF: Average Scores')\n",
    "ax2.set_ylabel('Average TF-IDF Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc7ce8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create TF-IDF DataFrame for better analysis\n",
    "tfidf_df = pd.DataFrame(tfidf_dense, columns=tfidf_feature_names)\n",
    "\n",
    "print(\"TF-IDF Matrix (first 5 documents, first 10 features):\")\n",
    "print(tfidf_df.iloc[:5, :10])\n",
    "\n",
    "# Find highest TF-IDF scores for each document\n",
    "for i in range(5):  # First 5 documents\n",
    "    doc_scores = tfidf_df.iloc[i].sort_values(ascending=False)\n",
    "    top_words = doc_scores[doc_scores > 0].head(5)\n",
    "    print(f\"\\nDocument {i+1}: '{df.iloc[i]['review']}'\")\n",
    "    print(\"Top 5 TF-IDF words:\")\n",
    "    for word, score in top_words.items():\n",
    "        print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1231dd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize sparse matrix structure\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# BoW matrix sparsity pattern\n",
    "ax1.spy(bow_matrix, markersize=2)\n",
    "ax1.set_title('BoW Matrix Sparsity Pattern')\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Documents')\n",
    "\n",
    "# TF-IDF matrix sparsity pattern\n",
    "ax2.spy(tfidf_matrix, markersize=2)\n",
    "ax2.set_title('TF-IDF Matrix Sparsity Pattern')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.set_ylabel('Documents')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show actual values in a heatmap (for small subset)\n",
    "plt.figure(figsize=(12, 8))\n",
    "subset_bow = bow_df.iloc[:10, :20]  # First 10 docs, first 20 features\n",
    "sns.heatmap(subset_bow, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('BoW Matrix Heatmap (Subset)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Documents')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b2ffdf",
   "metadata": {},
   "source": [
    "Working with N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802adf9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create vectorizers with bigrams\n",
    "bow_bigram = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),  # Include both unigrams and bigrams\n",
    "    max_features=200\n",
    ")\n",
    "\n",
    "tfidf_bigram = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=200\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "bow_bigram_matrix = bow_bigram.fit_transform(df['review'])\n",
    "tfidf_bigram_matrix = tfidf_bigram.fit_transform(df['review'])\n",
    "\n",
    "print(\"Bigram Analysis:\")\n",
    "print(f\"BoW with bigrams shape: {bow_bigram_matrix.shape}\")\n",
    "print(f\"TF-IDF with bigrams shape: {tfidf_bigram_matrix.shape}\")\n",
    "\n",
    "# Get bigram features\n",
    "bigram_features = bow_bigram.get_feature_names_out()\n",
    "bigrams_only = [feature for feature in bigram_features if ' ' in feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868fb7f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get bigram frequencies\n",
    "bow_bigram_dense = bow_bigram_matrix.toarray()\n",
    "bigram_frequencies = np.sum(bow_bigram_dense, axis=0)\n",
    "\n",
    "# Create DataFrame for bigram analysis\n",
    "bigram_freq_df = pd.DataFrame({\n",
    "    'ngram': bigram_features,\n",
    "    'frequency': bigram_frequencies,\n",
    "    'type': ['bigram' if ' ' in ngram else 'unigram' for ngram in bigram_features]\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "# Show top bigrams\n",
    "top_bigrams = bigram_freq_df[bigram_freq_df['type'] == 'bigram'].head(15)\n",
    "print(\"Top 15 Bigrams:\")\n",
    "print(top_bigrams)\n",
    "\n",
    "# Visualize top bigrams\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(top_bigrams)), top_bigrams['frequency'])\n",
    "plt.xticks(range(len(top_bigrams)), top_bigrams['ngram'], rotation=45, ha='right')\n",
    "plt.title('Top 15 Most Frequent Bigrams')\n",
    "plt.xlabel('Bigrams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79475a80",
   "metadata": {},
   "source": [
    " Feature Extraction for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b24fe3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['review'], df['sentiment'], test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Create different vectorizers for comparison\n",
    "vectorizers = {\n",
    "    'BoW': CountVectorizer(lowercase=True, stop_words='english', max_features=100),\n",
    "    'TF-IDF': TfidfVectorizer(lowercase=True, stop_words='english', max_features=100),\n",
    "    'BoW_Bigrams': CountVectorizer(lowercase=True, stop_words='english', \n",
    "                                   ngram_range=(1, 2), max_features=200),\n",
    "    'TF-IDF_Bigrams': TfidfVectorizer(lowercase=True, stop_words='english', \n",
    "                                      ngram_range=(1, 2), max_features=200)\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00b13a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train and evaluate models with different vectorizers\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Evaluating: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Vectorize training and test data\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"Training matrix shape: {X_train_vec.shape}\")\n",
    "    print(f\"Test matrix shape: {X_test_vec.shape}\")\n",
    "    \n",
    "    # Train Naive Bayes classifier\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'vectorizer': vectorizer,\n",
    "        'classifier': classifier,\n",
    "        'matrix_shape': X_train_vec.shape\n",
    "    }\n",
    "    \n",
    "    # Show classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ad547",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_results = pd.DataFrame({\n",
    "    'Method': list(results.keys()),\n",
    "    'Accuracy': [results[method]['accuracy'] for method in results.keys()],\n",
    "    'Features': [results[method]['matrix_shape'][1] for method in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(comparison_results)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(comparison_results['Method'], comparison_results['Accuracy'])\n",
    "ax1.set_title('Classification Accuracy by Feature Extraction Method')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xticklabels(comparison_results['Method'], rotation=45, ha='right')\n",
    "\n",
    "# Feature count comparison\n",
    "ax2.bar(comparison_results['Method'], comparison_results['Features'])\n",
    "ax2.set_title('Number of Features by Method')\n",
    "ax2.set_ylabel('Number of Features')\n",
    "ax2.set_xticklabels(comparison_results['Method'], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d6d17",
   "metadata": {},
   "source": [
    "Advanced Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb084be2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the best performing model\n",
    "best_method = max(results.keys(), key=lambda x: results[x]['accuracy'])\n",
    "best_vectorizer = results[best_method]['vectorizer']\n",
    "best_classifier = results[best_method]['classifier']\n",
    "\n",
    "print(f\"Best performing method: {best_method}\")\n",
    "print(f\"Best accuracy: {results[best_method]['accuracy']:.4f}\")\n",
    "\n",
    "# Get feature names and importance scores\n",
    "feature_names = best_vectorizer.get_feature_names_out()\n",
    "feature_importance = best_classifier.feature_log_prob_\n",
    "\n",
    "# Calculate feature importance difference (positive - negative class)\n",
    "importance_diff = feature_importance[1] - feature_importance[0]\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importance_diff\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 features for positive sentiment:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3952f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Top positive features\n",
    "top_positive = feature_importance_df.head(10)\n",
    "ax1.barh(range(len(top_positive)), top_positive['importance'])\n",
    "ax1.set_yticks(range(len(top_positive)))\n",
    "ax1.set_yticklabels(top_positive['feature'])\n",
    "ax1.set_title('Top 10 Features for Positive Sentiment')\n",
    "ax1.set_xlabel('Importance Score')\n",
    "\n",
    "# Top negative features\n",
    "top_negative = feature_importance_df.tail(10)\n",
    "ax2.barh(range(len(top_negative)), top_negative['importance'])\n",
    "ax2.set_yticks(range(len(top_negative)))\n",
    "ax2.set_yticklabels(top_negative['feature'])\n",
    "ax2.set_title('Top 10 Features for Negative Sentiment')\n",
    "ax2.set_xlabel('Importance Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d080d",
   "metadata": {},
   "source": [
    "Practical Applications and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df44ec5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test different parameters\n",
    "parameter_tests = [\n",
    "    {'max_features': 50, 'ngram_range': (1, 1)},\n",
    "    {'max_features': 100, 'ngram_range': (1, 1)},\n",
    "    {'max_features': 200, 'ngram_range': (1, 1)},\n",
    "    {'max_features': 100, 'ngram_range': (1, 2)},\n",
    "    {'max_features': 200, 'ngram_range': (1, 2)},\n",
    "    {'max_features': 300, 'ngram_range': (1, 2)}\n",
    "]\n",
    "\n",
    "tuning_results = []\n",
    "\n",
    "for params in parameter_tests:\n",
    "    # Create vectorizer with current parameters\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        max_features=params['max_features'],\n",
    "        ngram_range=params['ngram_range']\n",
    "    )\n",
    "    \n",
    "    # Vectorize data\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    tuning_results.append({\n",
    "        'max_features': params['max_features'],\n",
    "        'ngram_range': str(params['ngram_range']),\n",
    "        'accuracy': accuracy,\n",
    "        'matrix_shape': X_train_vec.shape\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "tuning_df = pd.DataFrame(tuning_results)\n",
    "print(\"Parameter Tuning Results:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0002b",
   "metadata": {},
   "source": [
    "Troubleshooting Common Issues\n",
    "\n",
    "Memory Issues\n",
    "If you encounter memory problems with large datasets:\n",
    "# Use max_features to limit vocabulary size\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "# Use min_df and max_df to filter rare/common words\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.95)\n",
    "\n",
    "Sparse Matrix Errors\n",
    "When working with sparse matrices:\n",
    "# Check if matrix is sparse\n",
    "if hasattr(matrix, 'toarray'):\n",
    "    dense_matrix = matrix.toarray()\n",
    "else:\n",
    "    dense_matrix = matrix\n",
    "    \n",
    "Vocabulary Mismatch\n",
    "When applying trained vectorizer to new data:\n",
    "# Always use transform() on new data, not fit_transform()\n",
    "X_new_vectorized = trained_vectorizer.transform(new_text_data)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
